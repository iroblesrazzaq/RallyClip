{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17029ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check for GPU devices\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpu_devices:\n",
    "    print(f\"‚úÖ GPU is available and TensorFlow is using it: {gpu_devices[0]}\")\n",
    "else:\n",
    "    print(\"‚ùå GPU not found. TensorFlow is using the CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911dfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = YOLO('yolov8n.pt')  # 'n' is the nano version, fast and small\n",
    "\n",
    "movenet_model = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "movenet_input_size = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_df = pd.read_csv(CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e81c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VIDEO_PATH = \"raw_videos/Monica Greene unedited tennis match play.mp4\"\n",
    "CSV_PATH = \"annotations/Monica Greene unedited tennis match play.mp4.csv\"\n",
    "\n",
    "# Load timestamps\n",
    "timestamps_df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(f\"‚úÖ Models loaded. Processing video: {VIDEO_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b88e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Pose Extraction\n",
    "# ==============================================================================\n",
    "def run_movenet(input_image):\n",
    "    \"\"\"Runs MoveNet on a single image and returns keypoints.\"\"\"\n",
    "    # Resize and pad the image to the model's expected input size\n",
    "    image_for_movenet = tf.image.resize_with_pad(\n",
    "        tf.expand_dims(input_image, axis=0), movenet_input_size, movenet_input_size\n",
    "    )\n",
    "    # Run inference\n",
    "    infer = movenet_model.signatures['serving_default']\n",
    "    # Run inference\n",
    "    outputs = infer(tf.cast(image_for_movenet, dtype=tf.int32))\n",
    "    # Access the output tensor from the returned dictionary\n",
    "    keypoints_with_scores = outputs['output_0']\n",
    "    return keypoints_with_scores\n",
    "\n",
    "# --- Main Loop ---\n",
    "video = cv2.VideoCapture(VIDEO_PATH)\n",
    "all_frame_data = []\n",
    "frame_count = 0\n",
    "\n",
    "print(\"üèÉ‚Äç‚ôÇÔ∏è Starting pose extraction from video...\")\n",
    "\n",
    "while video.isOpened():\n",
    "    success, frame = video.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # 1. Detect players with YOLO\n",
    "    results = yolo_model(frame, classes=[0], verbose=False) # class 0 is 'person'\n",
    "\n",
    "    # 2. Find the far-side player (highest bounding box in frame)\n",
    "    far_side_player_box = None\n",
    "    min_y = float('inf')\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n",
    "            if y1 < min_y:\n",
    "                min_y = y1\n",
    "                far_side_player_box = (x1, y1, x2, y2)\n",
    "\n",
    "    # 3. Get Pose if player is found\n",
    "    if far_side_player_box:\n",
    "        x1, y1, x2, y2 = far_side_player_box\n",
    "        player_crop = frame[y1:y2, x1:x2]\n",
    "\n",
    "        # Ensure crop is not empty\n",
    "        if player_crop.shape[0] > 0 and player_crop.shape[1] > 0:\n",
    "            keypoints_relative = run_movenet(player_crop)\n",
    "\n",
    "            # Convert keypoints to absolute coordinates\n",
    "            # MoveNet output is (y, x, score), so we need to adjust\n",
    "            kps = keypoints_relative[0, 0].numpy()\n",
    "            abs_kps = np.zeros_like(kps)\n",
    "            abs_kps[:, 0] = kps[:, 0] * (y2 - y1) + y1 # y-coordinate\n",
    "            abs_kps[:, 1] = kps[:, 1] * (x2 - x1) + x1 # x-coordinate\n",
    "            abs_kps[:, 2] = kps[:, 2] # score\n",
    "\n",
    "            all_frame_data.append({\n",
    "                \"frame_id\": frame_count,\n",
    "                \"keypoints\": abs_kps\n",
    "            })\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "video.release()\n",
    "print(f\"‚úÖ Pose extraction complete. Processed {frame_count} frames.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a370f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Feature Engineering\n",
    "# ==============================================================================\n",
    "print(\"Engineering features from pose data...\")\n",
    "\n",
    "# Create a dictionary for fast frame-to-keypoint lookup\n",
    "pose_lookup = {item['frame_id']: item['keypoints'] for item in all_frame_data}\n",
    "\n",
    "all_feature_data = []\n",
    "\n",
    "# Loop through all frames processed in the video\n",
    "for frame_id in range(frame_count):\n",
    "    if frame_id in pose_lookup and (frame_id - 1) in pose_lookup:\n",
    "        current_kps = pose_lookup[frame_id]\n",
    "        prev_kps = pose_lookup[frame_id - 1]\n",
    "\n",
    "        # Calculate velocity (change in position)\n",
    "        velocity = current_kps[:, :2] - prev_kps[:, :2]\n",
    "\n",
    "        # Create a feature vector: [pos_x, pos_y, vel_x, vel_y] for each keypoint\n",
    "        # Flatten the array to create a single feature vector per frame\n",
    "        feature_vector = np.concatenate([current_kps[:, :2].flatten(), velocity.flatten()])\n",
    "        \n",
    "        all_feature_data.append({\n",
    "            \"frame_id\": frame_id,\n",
    "            \"features\": feature_vector\n",
    "        })\n",
    "\n",
    "print(f\"‚úÖ Feature engineering complete. Processed {len(all_feature_data)} frames with features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328a202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create Labeled Sequences\n",
    "# ==============================================================================\n",
    "SEQUENCE_LENGTH = 60  # 60 frames = 2 seconds at 30fps\n",
    "STEP = 15             # Create a new sequence every 0.5 seconds\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Convert timestamps to frame numbers (assuming 30fps for video)\n",
    "# Note: You may need to get the actual FPS from the video if it's not 30\n",
    "fps = 30 # or video.get(cv2.CAP_PROP_FPS)\n",
    "timestamps_df['start_frame'] = timestamps_df['start_time'] * fps\n",
    "timestamps_df['end_frame'] = timestamps_df['end_time'] * fps\n",
    "\n",
    "def get_label_for_frame(frame_id, df):\n",
    "    \"\"\"Checks if a frame_id falls within an active point.\"\"\"\n",
    "    for _, row in df.iterrows():\n",
    "        if row['start_frame'] <= frame_id <= row['end_frame']:\n",
    "            return 1 # Active\n",
    "    return 0 # Inactive\n",
    "\n",
    "print(\"Creating training sequences...\")\n",
    "\n",
    "# Create a lookup dictionary for features\n",
    "feature_lookup = {item['frame_id']: item['features'] for item in all_feature_data}\n",
    "max_frame = max(feature_lookup.keys())\n",
    "\n",
    "for i in range(0, max_frame - SEQUENCE_LENGTH, STEP):\n",
    "    sequence = []\n",
    "    is_valid_sequence = True\n",
    "    for j in range(i, i + SEQUENCE_LENGTH):\n",
    "        if j in feature_lookup:\n",
    "            sequence.append(feature_lookup[j])\n",
    "        else:\n",
    "            # If a frame is missing features, this sequence is invalid\n",
    "            is_valid_sequence = False\n",
    "            break\n",
    "    \n",
    "    if is_valid_sequence:\n",
    "        X.append(sequence)\n",
    "        middle_frame_id = i + (SEQUENCE_LENGTH // 2)\n",
    "        y.append(get_label_for_frame(middle_frame_id, timestamps_df))\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(f\"‚úÖ Created training data. Shape of X: {X.shape}, Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9193db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Save Processed Data\n",
    "# ==============================================================================\n",
    "# Define a filename for your processed data\n",
    "DATA_FILENAME = \"processed_match_1.npz\"\n",
    "\n",
    "print(f\"üíæ Saving processed data to {DATA_FILENAME}...\")\n",
    "\n",
    "# Save both X and y arrays into a single compressed file\n",
    "np.savez_compressed(DATA_FILENAME, X=X, y=y)\n",
    "\n",
    "print(\"‚úÖ Data saved successfully. You can now restart the notebook and load this file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tennis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
